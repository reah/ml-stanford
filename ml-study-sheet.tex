%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% Machine Learning by Stanford University
% Professor Andrew Ng
% Study Sheet by Reah Miyara <mail@reah.me>
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb}
\usepackage{tikz}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
    decorations.pathreplacing,decorations.pathmorphing,shapes,%
    matrix,shapes.symbols}

\tikzset{
>=stealth',
  punktchain/.style={
    rectangle, 
    rounded corners, 
    % fill=black!10,
    draw=black, thick,
    text width=4em, 
    minimum height=1em, 
    text centered, 
    on chain},
  line/.style={draw, thin, <-},
  element/.style={
    tape,
    top color=white,
    bottom color=blue!50!black!60!,
    minimum width=5em,
    draw=blue!40!black!90, very thick,
    text width=7em, 
    minimum height=2.5em, 
    text centered, 
    on chain},
  every join/.style={->, thick,shorten >=1pt},
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage[misc]{ifsym}

\title{Machine Learning}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}

\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\newcommand{\sbt}{\,\begin{picture}(-1,1)(-1,-3)\circle*{3}\end{picture}\ }
\parindent0pt
\parskip2pt
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}
\begin{document}

\begin{center}{\huge{\textbf{Machine Learning by Stanford University}}}\\
	{\large \texttt{Study Sheet by Reah Miyara \href{mailto:ml@reah.me}{\Letter \hspace{3pt}ml@reah.me}}}
\end{center}
\begin{multicols*}{3}
	
	\tikzstyle{mybox} = [draw=black, fill=white, very thick,
	rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
	\tikzstyle{fancytitle} =[fill=white, text={rgb:yellow,2;green,4;blue,9}, font=\bfseries]
	%------------ Intro to Machine Learning ---------------
	\begin{tikzpicture}
		\node [mybox] (box){%
			\begin{minipage}{0.3\textwidth}
				\textbf{$\sbt$ ML} -- a computer program with increased performance $P$ at some class of tasks $T$ with experience $E$. \\
				\textbf{$\sbt$ Supervised} -- given a [`ground truth'] data set, predict output given the input. Types of prediction:
				\begin{enumerate}
					\itemsep0em 
					\item \textbf{Regression} -- continuous, numerical
					\item \textbf{Classification} -- discrete, categorical  
				\end{enumerate}
				        
				\textbf{$\sbt$ Unsupervised} -- derive structure from data based on relationships among variables (with no prior knowledge as to what the results should look like)
			\end{minipage}
		};
		\node[fancytitle, right=10pt] at (box.north west) {Intro to Machine Learning};
	\end{tikzpicture}
	
	%------------ Linear Regression with One Variable ---------------
	\begin{tikzpicture}
		\node [mybox] (box){%
			\begin{minipage}{0.3\textwidth}
				\textbf{$\sbt$ Learning Goal} -- given a training set, learn a function $h$: X$\rightarrow$Y so $h(x)$ is a good $y$ predictor
				    
				\begin{center}
					\tiny\begin{tikzpicture}
					[node distance=.5cm,
					start chain=going below,]
					\node[punktchain, join] (perfekt) {Training Set};
					\node[punktchain, join, ] (emperi) {Learning Algorithm};
					\node (asym) [punktchain, join ]  {function $h$: X$\rightarrow$Y};
					\begin{scope}[start branch=venstre,
							%We need to redefine the join-style to have the -> turn out right
						every join/.style={->, thick, shorten <=1pt}, ]
						\node[punktchain, on chain=going left, join=by {<-}]
						() {X};
					\end{scope}
					\begin{scope}[start branch=hoejre,]
						\node () [punktchain, on chain=going right, join=by {->}] {prediction Y};
					\end{scope}
					\end{tikzpicture}
				\end{center}
				  
				\textbf{$\sbt$ Hypothesis} -- $h_\theta(x) = \theta_0 + \theta_1x$ \\
				\textbf{$\sbt$ Cost Function} -- takes an average difference of all results of the hypothesis with inputs from the $x$ values and the actual $y$ values. Goal: minimize $\theta_0,\theta_1$\vspace{-10pt}
				\begin{equation}
				J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)^2 
				\vspace{-5pt}\end{equation}
				{\tiny(1) Squared Error function or Mean Squared Error function}\\
				\textbf{$\sbt$ Gradient Descent Algorithm} {\tiny repeat until convergence}
				\vspace{-10pt}\begin{equation}
				\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)
				\end{equation}
			\end{minipage}
		};
		\node[fancytitle, right=10pt] at (box.north west) {Linear Regression with One Variable};
	\end{tikzpicture}
	
	
	\columnbreak
	
%------------ Multivariate Linear Regression ---------------
	\begin{tikzpicture}
		\node [mybox] (box){%
			\begin{minipage}{0.3\textwidth}
				$h_{\theta}(x)$ = 
				$\begin{bmatrix}
					\theta_{0} & \theta_{1} & \dots & \theta_{n}
				\end{bmatrix}$
				$\begin{bmatrix}
					x_{0}\\ 
					x_{1}\\ 
					\vdots\\ 
					x_{n}
				\end{bmatrix}$
				= $\theta^{T}x$
				
				\textbf{$\sbt$ Gradient Descent Algorithm} {\tiny repeat until convergence}
				\vspace{-10pt}\begin{equation}
				\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}-y^{(i)})\cdot x_j^{(i)} \text{ ; j:= 0\dots n} 
				\end{equation}
				
				\vspace{-5pt}
				\textbf{$\sbt$ Feature Scaling} -- divide the input values by the range (max -- min). Input values in roughly the same range speed up the convergence of gradient descent.\\
				\textbf{$\sbt$ Mean Normalization} -- subtract the mean for an input variable from the values for that input variable.
				\vspace{-8pt}\begin{equation}
				    x_i := \frac{x_i - \mu_i}{s_i}
				\end{equation}
				{\tiny(4) $\mu_i$ is the mean \& $s_i$ is the range, $(max-min)$, of all values for feature $i$ \par}
				
				\textbf{$\sbt$ Learning Rate} -- $\alpha$ too small $\implies$ slow convergence; 
				    $\alpha$ too large $\implies$ may not converge.
			\end{minipage}
			};
		\node[fancytitle, right=10pt] at (box.north west) {Multivariate Linear Regression};
	\end{tikzpicture}
	
%------------ Normal Equation ---------------
	\begin{tikzpicture}
		\node [mybox] (box){%
			\begin{minipage}{0.3\textwidth}
				\textbf{$\sbt$ Normal Equation} -- non-iterative algorithm for minimizing $J(\theta)$; $note:$ O($n^3$)\ to calculate $X^{T}X$\vspace{-8pt}
				\begin{equation}
				    \theta = (X^TX)^{-1}X^Ty
				\end{equation}
				$m$ examples $(x^{(1)},y^{(1)}), \dots, (x^{(m)},y^{(m)})$; $n$ features\\
               			$x^{(i)}$ = 
                			$\begin{bmatrix}
					x^{(i)}_{0}\\
					x^{(i)}_{1}\\
					\vdots\\
					x^{(i)}_{n}
				\end{bmatrix}$
				$\in \mathbb{R}^{n+1}$
				\hspace{3pt}$\vert$\hspace{3pt} 
				X= 
				$\begin{bmatrix}
					(x^{(1)})^T\\
					(x^{(2)})^T\\
					\vdots\\
					(x^{(n)})^T 
				\end{bmatrix}$
				\hspace{3pt}$\vert$\hspace{3pt} 
				y = $\begin{bmatrix}
					y^1\\
					y^2\\
					\vdots\\
					y^m 
				\end{bmatrix}$
				
		{\tiny $x^{(i)}$ = training vector $i$ (containing values from all features); $X$ $\rightarrow$ $m$x(n+1)}\\
		\textbf{$\sbt$ If X$^{T}$X is noninvertible}, common causes include:\par
		\begin{enumerate}
			\itemsep0em 
			\item Redundant features, where two features are very closely related (i.e. they are linearly dependent)
			\item Too many features (e.g. m $\leq$ n). In this case, delete some features or use `regularization'.
		\end{enumerate}
				
			\end{minipage}
		};
		\node[fancytitle, right=10pt] at (box.north west) {Normal Equation};
	\end{tikzpicture}
	

%------------ Classification Logistic Regression ---------------
	\begin{tikzpicture}
		\node [mybox] (box){%
			\begin{minipage}{0.3\textwidth}
				\textbf{$\sbt$ Logistic Regression Model} -- want 0 $\leq$ $h_{\theta}(x) \leq 1$\\
				\vspace{-10pt}\begin{equation}
				    g(z) = \frac{1}{1+e^{-z}}
				\end{equation}
				\textbf{$\sbt$ Sigmoid/Logistic Function}
				\vspace{-10pt}\begin{equation}
				 h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}(x)}} 
				 \end{equation}
				 \vspace{-20pt}\begin{equation}
				     h_{\theta}(x) = P(y=1 \vert x; \theta) = 1 - P(y=0 \vert x;\theta)
				\end{equation}
		\textbf{$\sbt$ Decision Boundary}\par
		\begin{center}\begin{enumerate}
    			\itemsep0em 
    			\item y = 1 if $\theta^{T}(x) \geq 0$
			\item y = 0 if $\theta^{T}(x) < 0 $
		\end{enumerate}
		\end{center}
		\textbf{$\sbt$ Cost Function}\par
		    \vspace{-20pt}\begin{equation}
			Cost (h_{\theta}(x), y) =
			\begin{cases} 
              -$log$(h_{\theta}(x))& $if $y=1 \\
              -$log$(1-h_{\theta}(x))& $if $y=0 \\
            \end{cases}
            \end{equation}\\\vspace{-30pt}
            \begin{equation}
		Cost(h_\theta(x), y) = -ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
	    \end{equation}
            \vspace{-15pt}\begin{equation}
			    J(\theta)=\frac{1}{m}\sum_{i = 1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})
			  \vspace{-8pt}\end{equation}
			{\tiny a vectorized implementation is:}\\
			 \vspace{-20pt}\begin{equation}
			    h=g(X\theta); \hspace{10pt} 
			    J(\theta)=\frac{1}{m} \cdot (-y^{T}log(h)-(1-y)^{T}log(1-h))
			 \end{equation}
			 
			 \vspace{-10pt}\begin{equation}
			    \theta_{j} := \theta_{j} - \frac{\alpha}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})\cdot x_{j}^{(i)}
			 \end{equation}
			 {\tiny a vectorized implementation is:}\\
			 \vspace{-10pt}\begin{equation}
			    \theta := \theta - \frac{\alpha}{m} X^{T}(g(X\theta)-\vec{y} )
			    \end{equation}
			\end{minipage}
		};
		\node[fancytitle, right=10pt] at (box.north west) {Classification Logistic Regression};
	\end{tikzpicture}
	
	%------------ Multiclass Classification ---------------
	\begin{tikzpicture}
		\node [mybox] (box){%
			\begin{minipage}{0.3\textwidth}
			\begin{center}
			    y $\in {0, 1, \dots n}$\\
			    h$_\theta^{(0)}(x)$ = P(y=0$\vert$x;$\theta$)\\
			    h$_\theta^{(1)}(x)$ = P(y=1$\vert$x;$\theta$)\\\vspace{-5pt}
			    $\vdots$\\
			    h$_\theta^{(n)}(x)$ = P(y=n$\vert$x;$\theta$)\\
			    prediction = max$_{i}(h_{\theta}^{(i)}(x))$
			\end{center}
			\end{minipage}
		};
		\node[fancytitle, right=10pt] at (box.north west) {Multiclass Classification};
	\end{tikzpicture}
	
	\columnbreak
		%------------ Overfitting ---------------
	\begin{tikzpicture}
		\node [mybox] (box){%
			\begin{minipage}{0.3\textwidth}
			\textbf{$\sbt$ Underfitting} -- hypothesis shows structure not captured by the model; does not fit the data well\\
			\textbf{$\sbt$ Overfitting} -- hypothesis corresponds too closely to data $\therefore$ fail to predict future results reliably\\
			$\sbt$ Addressing the problem of overfitting: \\
			\vspace{-20pt}\begin{enumerate}
			\itemsep0em
			    \item \textbf{Reduce the number of features} -- manually/algorithmically select subset of features.
			    \vspace{-5pt}\item \textbf{Regularization} -- keep all features, but reduce the magnitude of parameters $\theta_j$ by $\lambda$ {\tiny (regularization parameter)}.
			\end{enumerate}
			\vspace{-10pt}\begin{equation}
			        J(\theta) = \frac{1}{2m}\left[ \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda\sum_{j=1}^{n}\theta^2_j \right]
			\end{equation}
			Gradient Descent {\tiny regularized linear regression does not penalize} $\theta_0$
			\vspace{-10pt}\begin{equation}
		        		\theta_0 := \theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})\cdot x^{(i)}_0
			\end{equation}
			\vspace{-10pt}\begin{equation}
		        		\theta_j := \theta_j-\alpha \left[ \left( \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})\cdot x^{(i)}_j \right) + \frac{\lambda}{m}\theta_j \right]
			\end{equation}
			\vspace{-15pt}\begin{equation}
		        		\theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})\cdot x^{(i)}_j 
			\end{equation}

			Normal Equation
			\vspace{-10pt}\begin{equation}
		        		\theta = (X^TX + \lambda \cdot L)^{-1}X^Ty
			\end{equation}
			where L = 
			$\begin{bmatrix}
    				0 & \\
    				& 1 \\
				& & \ddots \\
    				& & & 1
 			\end{bmatrix}$
			\end{minipage}
		};
		\node[fancytitle, right=10pt] at (box.north west) {Under/Over --fitting};
	\end{tikzpicture}

	
	
		\end{multicols*}
\end{document}
